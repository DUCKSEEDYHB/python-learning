# -*- coding: utf-8 -*-
import requests
import time
import json
import os
import re
import random
import pandas as pd
from requests.adapters import HTTPAdapter
from urllib3.util.retry import Retry

# ===================== æ ¸å¿ƒé…ç½®ï¼ˆç²¾å‡†åŒ¹é…éœ€æ±‚ï¼‰ =====================
SAVE_DIR = r"E:\py\practice\zpy"          
PDF_SAVE_DIR = os.path.join(SAVE_DIR, "pdf")  
MAX_PAGE = 311                             # ç½‘é¡µå®é™…æ€»é¡µæ•°ï¼ˆç²¾å‡†çˆ¬å–æ— æ— æ•ˆé¡µï¼‰
TARGET_DATA_COUNT = 7762                   # ç›®æ ‡çˆ¬å–å…¨éƒ¨7762æ¡
BATCH_SIZE = 20                            
RETRY_TIMES = 3                            
MIN_DELAY = 1.5                            
MAX_DELAY = 3                              
RESUME_FILE = os.path.join(SAVE_DIR, "resume.txt")  
EXCEL_FILE = os.path.join(SAVE_DIR, "ä¸Šäº¤æ‰€å€ºåˆ¸å‘è¡Œå…¬å‘Šæ•°æ®.xlsx")  

# ===================== åˆå§‹åŒ– =====================
os.makedirs(SAVE_DIR, exist_ok=True)
os.makedirs(PDF_SAVE_DIR, exist_ok=True)

code = []
short_name = []
name = []
set_time = []
pdf_names = []

# è¯·æ±‚ä¼šè¯ä¼˜åŒ–
session = requests.Session()
retry_strategy = Retry(
    total=RETRY_TIMES,
    backoff_factor=0.8,
    status_forcelist=[429, 500, 502, 503, 504],
    allowed_methods=["GET"]
)
session.mount("https://", HTTPAdapter(max_retries=retry_strategy, pool_connections=10))

USER_AGENTS = [
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/142.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/142.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/141.0.0.0 Safari/537.36',
    'Mozilla/5.0 (Windows NT 10.0; Win64; x64) Firefox/139.0'
]

# è¯»å–æ–­ç‚¹
start_page = 1
if os.path.exists(RESUME_FILE):
    with open(RESUME_FILE, "r", encoding="utf-8") as f:
        resume_page = f.read().strip()
        if resume_page.isdigit() and int(resume_page) > 1:
            start_page = int(resume_page)
    print(f"ã€æ–­ç‚¹ç»­çˆ¬ã€‘ä»ç¬¬ {start_page} é¡µå¼€å§‹")
else:
    print(f"ã€é¦–æ¬¡çˆ¬å–ã€‘ä»ç¬¬ 1 é¡µå¼€å§‹ï¼Œç›®æ ‡ {TARGET_DATA_COUNT} æ¡å‘è¡Œå…¬å‘Š")

# ===================== å·¥å…·å‡½æ•°ï¼šä¸‹è½½PDFï¼ˆæ ¸å¿ƒä¿®æ­£ï¼‰ =====================
def download_pdf(pdf_url, save_name):
    """
    ç¡®ä¿ä¸‹è½½çš„PDFæ–‡ä»¶å = å€ºåˆ¸ç®€ç§°_å…¬å‘Šæ ‡é¢˜.pdf
    :param save_name: ä¼ å…¥çš„æ˜¯å·²ç»æŒ‰â€œå€ºåˆ¸ç®€ç§°_å…¬å‘Šæ ‡é¢˜â€ç”Ÿæˆçš„åç§°
    """
    # ä»…å¤„ç†ç‰¹æ®Šå­—ç¬¦ï¼Œä¸ä¿®æ”¹æ ¸å¿ƒå‘½åè§„åˆ™ï¼ˆå’ŒExcelä¸­PDFåç§°å®Œå…¨ä¸€è‡´ï¼‰
    safe_name = re.sub(r'[\\/:*?"<>|]', '_', save_name)
    save_path = os.path.join(PDF_SAVE_DIR, f"{safe_name}.pdf")

    # å·²å­˜åœ¨åˆ™è·³è¿‡
    if os.path.exists(save_path):
        print(f"ğŸ“„ å·²å­˜åœ¨ï¼š{safe_name}.pdf")
        return "å·²å­˜åœ¨"

    # æ— é“¾æ¥åˆ™æ ‡è®°
    if not pdf_url:
        print(f"âš ï¸  æ— é“¾æ¥ï¼š{safe_name}")
        return "æ— é“¾æ¥"

    # ä¸‹è½½é€»è¾‘
    try:
        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Referer': 'https://www.sse.com.cn/',
            'Accept': 'application/pdf, */*'
        }
        response = session.get(pdf_url, headers=headers, timeout=20)
        response.raise_for_status()

        with open(save_path, 'wb') as f:
            f.write(response.content)
        print(f"âœ… ä¸‹è½½æˆåŠŸï¼š{safe_name}.pdf")
        return "æˆåŠŸ"
    except Exception as e:
        print(f"âŒ ä¸‹è½½å¤±è´¥ï¼š{safe_name} - {str(e)[:50]}")
        return f"å¤±è´¥ï¼š{str(e)[:20]}"

# ===================== æ ¸å¿ƒçˆ¬å–å‡½æ•°ï¼ˆç²¾å‡†ç­›é€‰ï¼‰ =====================
def crawl_page(page_num):
    """
    ç²¾å‡†ç­›é€‰ï¼š
    1. æ¥å£å±‚ç­›é€‰åˆ†ç±»ä¸ºå‘è¡Œå…¬å‘Š
    2. æœ¬åœ°ç­›é€‰æ ‡é¢˜å«è¿ç»­"å‘è¡Œå…¬å‘Š"å››å­—
    """
    print(f"\nã€è¯·æ±‚ä¸­ã€‘ç¬¬ {page_num} é¡µ")
    try:
        params = {
            'jsonCallBack': f'jsonCallback{random.randint(10000000, 99999999)}',
            'isPagination': 'true',
            'pageHelp.pageSize': '25',
            'pageHelp.cacheSize': '1',
            'type': 'inParams',
            'sqlId': 'BS_ZQ_GGLL',
            'sseDate': '2020-01-01 00:00:00',
            'sseDateEnd': '2024-12-31 23:59:59',
            'securityCode': '',
            'title': 'å‘è¡Œå…¬å‘Š',  # æ¥å£å±‚ç­›é€‰åˆ†ç±»ä¸ºå‘è¡Œå…¬å‘Š
            'orgBulletinType': '1101',
            'bondType': 'COMPANY_BOND_BULLETIN',
            'order': 'sseDate|desc,securityCode|asc,bulletinId|asc',
            'pageHelp.pageNo': str(page_num),
            'pageHelp.beginPage': str(page_num),
            'pageHelp.endPage': str(page_num),
            '_': str(int(time.time() * 1000))
        }

        headers = {
            'User-Agent': random.choice(USER_AGENTS),
            'Referer': 'https://www.sse.com.cn/',
            'Host': 'query.sse.com.cn'
        }

        url = 'https://query.sse.com.cn/commonSoaQuery.do'
        response = session.get(url, params=params, headers=headers, timeout=15)
        response.raise_for_status()

        # è§£æJSON
        json_text = response.text[response.text.index('(')+1 : response.text.rindex(')')]
        data = json.loads(json_text)
        page_data = data["pageHelp"]["data"]

        # æœ¬åœ°ç²¾å‡†ç­›é€‰ï¼šæ ‡é¢˜åŒ…å«è¿ç»­çš„"å‘è¡Œå…¬å‘Š"å››å­—
        result = []
        for item in page_data:
            title = item.get("title", "").strip()
            # æ ¸å¿ƒè§„åˆ™ï¼šæ ‡é¢˜ä¸­å­˜åœ¨è¿ç»­çš„"å‘è¡Œå…¬å‘Š"ï¼ˆä¸é™åˆ¶ä½ç½®/ç»“å°¾ï¼‰
            if "å‘è¡Œå…¬å‘Š" in title:
                pdf_relative_url = item.get("url", "")
                full_pdf_url = f"https://static.sse.com.cn{pdf_relative_url}" if pdf_relative_url else ""
                # ç»Ÿä¸€å‘½åè§„åˆ™ï¼šå€ºåˆ¸ç®€ç§°_å…¬å‘Šæ ‡é¢˜ï¼ˆå’Œä¸‹è½½çš„PDFæ–‡ä»¶åå®Œå…¨ä¸€è‡´ï¼‰
                pdf_file_name = f"{item.get('securityAbbr', '')}_{title}"
                
                result.append({
                    "è¯åˆ¸ä»£ç ": item.get("securityCode", ""),
                    "è¯åˆ¸ç®€ç§°": item.get("securityAbbr", ""),
                    "å…¬å‘Šæ ‡é¢˜": title,
                    "å‘å¸ƒæ—¥æœŸ": item.get("sseDate", "")[:10],
                    "PDFé“¾æ¥": full_pdf_url,
                    "PDFåç§°": pdf_file_name  # Excelä¸­è®°å½•çš„åç§°
                })
        print(f"ã€è§£æå®Œæˆã€‘ç¬¬ {page_num} é¡µï¼š{len(result)} æ¡ç¬¦åˆæ¡ä»¶çš„å‘è¡Œå…¬å‘Š")
        return result

    except Exception as e:
        print(f"ã€çˆ¬å–å¤±è´¥ã€‘ç¬¬ {page_num} é¡µï¼š{str(e)}")
        return None

# ===================== ä¸»é€»è¾‘ï¼ˆç¡®ä¿çˆ¬æ»¡7762æ¡ï¼‰ =====================
if __name__ == "__main__":
    is_stop = False  # ç»ˆæ­¢æ ‡è®°
    try:
        for page in range(start_page, MAX_PAGE + 1):
            if is_stop:
                break
                
            # çˆ¬å–å½“å‰é¡µ
            page_result = crawl_page(page)
            if not page_result:
                time.sleep(random.uniform(MIN_DELAY, MAX_DELAY))
                continue

            # å¤„ç†å½“å‰é¡µæ‰€æœ‰æ•°æ®ï¼ˆé¿å…æ¼çˆ¬ï¼‰
            new_count = 0
            for item in page_result:
                # å…ˆè¿½åŠ æ•°æ®ï¼Œå†åˆ¤æ–­æ˜¯å¦ç»ˆæ­¢
                code.append(item["è¯åˆ¸ä»£ç "])
                short_name.append(item["è¯åˆ¸ç®€ç§°"])
                name.append(item["å…¬å‘Šæ ‡é¢˜"])
                set_time.append(item["å‘å¸ƒæ—¥æœŸ"])
                pdf_names.append(item["PDFåç§°"])  # è®°å½•ç»Ÿä¸€çš„å‘½å

                # ä¸‹è½½PDFï¼šç›´æ¥ä¼ å…¥ç»Ÿä¸€å‘½åçš„åç§°ï¼Œç¡®ä¿æ–‡ä»¶åä¸€è‡´
                download_status = download_pdf(item["PDFé“¾æ¥"], item["PDFåç§°"])
                new_count += 1

                # è¾¾åˆ°ç›®æ ‡é‡ï¼Œæ ‡è®°ç»ˆæ­¢
                if len(code) >= TARGET_DATA_COUNT:
                    is_stop = True
                    break

            # æ‰“å°è¿›åº¦
            print(f"âœ… ç´¯è®¡ï¼š{len(code)} æ¡ï¼ˆæ–°å¢ {new_count} æ¡ï¼‰")

            # åˆ†æ‰¹ä¿å­˜
            if (page % BATCH_SIZE == 0) or (page == MAX_PAGE) or is_stop:
                df = pd.DataFrame({
                    "è¯åˆ¸ä»£ç ": code,
                    "è¯åˆ¸ç®€ç§°": short_name,
                    "å…¬å‘Šæ ‡é¢˜": name,
                    "å‘å¸ƒæ—¥æœŸ": set_time,
                    "PDFåç§°": pdf_names  # Excelä¸­æ˜¾ç¤ºçš„åç§°å’Œä¸‹è½½çš„PDFæ–‡ä»¶åä¸€è‡´
                })
                df.to_excel(EXCEL_FILE, index=False, engine='openpyxl')
                print(f"ğŸ’¾ å·²ä¿å­˜åˆ°ï¼š{EXCEL_FILE}")

                # è®°å½•æ–­ç‚¹
                with open(RESUME_FILE, "w", encoding="utf-8") as f:
                    f.write(str(page + 1))
                print(f"ğŸ“Œ æ–­ç‚¹ï¼šä¸‹æ¬¡ä»ç¬¬ {page + 1} é¡µå¼€å§‹")

            # éšæœºå»¶è¿Ÿ
            time.sleep(random.uniform(MIN_DELAY, MAX_DELAY))

        # æœ€ç»ˆä¿å­˜
        df_final = pd.DataFrame({
            "è¯åˆ¸ä»£ç ": code,
            "è¯åˆ¸ç®€ç§°": short_name,
            "å…¬å‘Šæ ‡é¢˜": name,
            "å‘å¸ƒæ—¥æœŸ": set_time,
            "PDFåç§°": pdf_names
        })
        df_final.to_excel(EXCEL_FILE, index=False, engine='openpyxl')

        # çˆ¬å–å®Œæˆï¼Œæ¸…ç†æ–­ç‚¹æ–‡ä»¶
        if os.path.exists(RESUME_FILE):
            os.remove(RESUME_FILE)

        print(f"\nğŸ“Š æœ€ç»ˆç»“æœï¼šå…±çˆ¬å– {len(code)} æ¡ç¬¦åˆæ¡ä»¶çš„å‘è¡Œå…¬å‘Š")
        print(f"ğŸ“ Excelæ–‡ä»¶ï¼š{EXCEL_FILE}")
        print(f"ğŸ“‚ PDFç›®å½•ï¼š{PDF_SAVE_DIR}")

    except KeyboardInterrupt:
        # æ‰‹åŠ¨ä¸­æ–­æ—¶ä¿å­˜
        df = pd.DataFrame({
            "è¯åˆ¸ä»£ç ": code,
            "è¯åˆ¸ç®€ç§°": short_name,
            "å…¬å‘Šæ ‡é¢˜": name,
            "å‘å¸ƒæ—¥æœŸ": set_time,
            "PDFåç§°": pdf_names
        })
        df.to_excel(EXCEL_FILE, index=False, engine='openpyxl')
        print(f"\nâš ï¸  æ‰‹åŠ¨ä¸­æ–­ï¼Œå·²ä¿å­˜ {len(code)} æ¡")

    finally:
        session.close()
        print("ğŸ”Œ ä¼šè¯å…³é—­ï¼Œç¨‹åºç»“æŸ")